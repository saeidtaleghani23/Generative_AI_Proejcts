# ğŸ¨ CycleGAN for Image-to-Image Translation

This repository contains an implementation of **CycleGAN**, an unpaired image-to-image translation model. The model learns to translate images between two domains **without requiring paired training data**.

## ğŸ“Œ Project Overview
- **Model Type:** CycleGAN (Generative Adversarial Network)
- **Number of Generators:** 2 (Generator A & Generator B)
- **Number of Discriminators:** 2 (Discriminator A & Discriminator B)
- **Key Concept:** **Cycle Consistency Loss** ensures that translations preserve the original content.

---

## ğŸ–¼ï¸ How CycleGAN Works

| Domain | Generator | Discriminator |
|--------|-----------|--------------|
| **Domain 1 â†’ Domain 2** | **Generator A** converts images from **Domain 1 to Domain 2**. | **Discriminator A** classifies real vs. fake images from Domain 2. |
| **Domain 2 â†’ Domain 1** | **Generator B** converts images from **Domain 2 to Domain 1**. | **Discriminator B** classifies real vs. fake images from Domain 1. |

### **ğŸ”„ Cycle Consistency Training**
- **Forward Cycle:** \( \text{Domain1} \rightarrow \text{Generator A} \rightarrow \text{Domain2} \rightarrow \text{Generator B} \rightarrow \text{Domain1} \)
- **Backward Cycle:** \( \text{Domain2} \rightarrow \text{Generator B} \rightarrow \text{Domain1} \rightarrow \text{Generator A} \rightarrow \text{Domain2} \)

---

## ğŸ›  Installation Guide

To replicate this project, set up the environment using Conda.

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/saeidtaleghani23/CycleGAN.git
cd CycleGAN
```

### **2ï¸âƒ£ Create & Activate Conda Environment
```bash
conda env create -f env/env.yml
conda activate cyclegan
```

### **3ï¸âƒ£ Verify Installation
Check installed package versions:
```bash
python -c "import tensorflow as tf; import numpy as np; import matplotlib; print(tf.__version__, np.__version__, matplotlib.__version__)"

```

## ğŸ“Š Dataset Information

The model is trained on the **[CycleGAN Dataset](http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/)**.  
Available dataset options include:  

- **`apple2orange`** ğŸ â†’ ğŸŠ  
- **`summer2winter_yosemite`** â˜€ï¸ â†’ â„ï¸  
- **`horse2zebra`** ğŸ´ â†’ ğŸ¦“  
- **`vangogh2photo`** ğŸ¨ â†’ ğŸ“· (default)  

### **ğŸ“¥ Download Dataset**
Modify `dataset_name` inside the code to select a different dataset:  

```python
dataset_name = 'vangogh2photo'  # Change this to any dataset you want
DOWNLOAD_URL = f'http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/{dataset_name}.zip'

## ğŸš€ Running the Model  

All training and image generation steps are implemented in the Jupyter Notebook: [`CycleGAN.ipynb`](CycleGAN.ipynb).  

### **How to Run the Notebook**
1. Open the Jupyter Notebook:  
   ```bash
   jupyter notebook main.ipynb
   ```










# General description of CycleGAN Model
- <small> The model contains two generator models: </small>
     - <small> Generator A: Generates images for the first domain. </small>
    - <small> Generator B: Generates images for the second domain. </small>
- <small> Inputs to generators come from the other domain: </small>
     - <small> Domain1 â¡ Generator A â¡ Domain2 </small>
     - <small> Domain2 â¡ Generator B â¡ Domain1 </small>

- <small> Each generator has a corresponding discriminator model (Discriminator A and Discriminator B): </small>
     - <small> Domain2 â¡ Discriminator A â¡ real/fake </small>
     - <small> Domain1 â¡ Generator A  (Generates image in Domain 2) â¡ Discriminator A â¡ real/fake </small>
     - <small> Domain1 â¡ Discriminator B â¡ real/fake </small>
     - <small> Domain2 â¡ Generator B  (Generates image in Domain 1) â¡ Discriminator B â¡ real/fake </small>

- <small> Cycle consistency: Generator models are trained to reproduce the original source image. </small>
     - <small>Use generated image as input to the corresponding generator model and compare the output image to the original image. </small>
      <small> Domain1 â¡ Generator A  (Generates image in Domain 2) â¡ Generator B (Generates image in Domain 1) </small>
      <small> Domain2 â¡ Generator B  (Generates image in Domain 1) â¡ Generator A (Generates image in Domain 2) </small>

- <small> Generator is trained via the combined model to minimize 4 different losses</small>
    - <small> Adversarial loss (L2- MSE): Minimize the loss predicted by the discriminator for generated images marked as "real"</small>
    - <small> Identity loss (L1- MAE): Output the source image as-is without translation.</small>
    - <small> Cycle loss forward (L1- MAE): Regeneration of a source image when used with the other model.</small>
    - <small> Cycle loss backward (L1- MAE): Regeneration of a source image when used with the other model.</small>

    