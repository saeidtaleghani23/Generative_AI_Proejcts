# 🎨 CycleGAN for Image-to-Image Translation

This repository contains an implementation of **CycleGAN**, an unpaired image-to-image translation model. The model learns to translate images between two domains **without requiring paired training data**.

## 📌 Project Overview
- **Model Type:** CycleGAN (Generative Adversarial Network)
- **Number of Generators:** 2 (Generator A & Generator B)
- **Number of Discriminators:** 2 (Discriminator A & Discriminator B)
- **Key Concept:** **Cycle Consistency Loss** ensures that translations preserve the original content.

---

## 🖼️ How CycleGAN Works

| Domain | Generator | Discriminator |
|--------|-----------|--------------|
| **Domain 1 → Domain 2** | **Generator A** converts images from **Domain 1 to Domain 2**. | **Discriminator A** classifies real vs. fake images from Domain 2. |
| **Domain 2 → Domain 1** | **Generator B** converts images from **Domain 2 to Domain 1**. | **Discriminator B** classifies real vs. fake images from Domain 1. |

### **🔄 Cycle Consistency Training**
- **Forward Cycle:** \( \text{Domain1} \rightarrow \text{Generator A} \rightarrow \text{Domain2} \rightarrow \text{Generator B} \rightarrow \text{Domain1} \)
- **Backward Cycle:** \( \text{Domain2} \rightarrow \text{Generator B} \rightarrow \text{Domain1} \rightarrow \text{Generator A} \rightarrow \text{Domain2} \)

---

## 🛠 Installation Guide

To replicate this project, set up the environment using Conda.

### **1️⃣ Clone the Repository**
```bash
git clone https://github.com/saeidtaleghani23/CycleGAN.git
cd CycleGAN
```

### **2️⃣ Create & Activate Conda Environment
```bash
conda env create -f env/env.yml
conda activate cyclegan
```

### **3️⃣ Verify Installation
Check installed package versions:
```bash
python -c "import tensorflow as tf; import numpy as np; import matplotlib; print(tf.__version__, np.__version__, matplotlib.__version__)"

```

## 📊 Dataset Information

The model is trained on the **[CycleGAN Dataset](http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/)**.  
Available dataset options include:  

- **`apple2orange`** 🍏 → 🍊  
- **`summer2winter_yosemite`** ☀️ → ❄️  
- **`horse2zebra`** 🐴 → 🦓  
- **`vangogh2photo`** 🎨 → 📷 (default)  

### **📥 Download Dataset**
Modify `dataset_name` inside the code to select a different dataset:  

```python
dataset_name = 'vangogh2photo'  # Change this to any dataset you want
DOWNLOAD_URL = f'http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/{dataset_name}.zip'

## 🚀 Running the Model  

All training and image generation steps are implemented in the Jupyter Notebook: [`CycleGAN.ipynb`](CycleGAN.ipynb).  

### **How to Run the Notebook**
1. Open the Jupyter Notebook:  
   ```bash
   jupyter notebook main.ipynb
   ```










# General description of CycleGAN Model
- <small> The model contains two generator models: </small>
     - <small> Generator A: Generates images for the first domain. </small>
    - <small> Generator B: Generates images for the second domain. </small>
- <small> Inputs to generators come from the other domain: </small>
     - <small> Domain1 ➡ Generator A ➡ Domain2 </small>
     - <small> Domain2 ➡ Generator B ➡ Domain1 </small>

- <small> Each generator has a corresponding discriminator model (Discriminator A and Discriminator B): </small>
     - <small> Domain2 ➡ Discriminator A ➡ real/fake </small>
     - <small> Domain1 ➡ Generator A  (Generates image in Domain 2) ➡ Discriminator A ➡ real/fake </small>
     - <small> Domain1 ➡ Discriminator B ➡ real/fake </small>
     - <small> Domain2 ➡ Generator B  (Generates image in Domain 1) ➡ Discriminator B ➡ real/fake </small>

- <small> Cycle consistency: Generator models are trained to reproduce the original source image. </small>
     - <small>Use generated image as input to the corresponding generator model and compare the output image to the original image. </small>
      <small> Domain1 ➡ Generator A  (Generates image in Domain 2) ➡ Generator B (Generates image in Domain 1) </small>
      <small> Domain2 ➡ Generator B  (Generates image in Domain 1) ➡ Generator A (Generates image in Domain 2) </small>

- <small> Generator is trained via the combined model to minimize 4 different losses</small>
    - <small> Adversarial loss (L2- MSE): Minimize the loss predicted by the discriminator for generated images marked as "real"</small>
    - <small> Identity loss (L1- MAE): Output the source image as-is without translation.</small>
    - <small> Cycle loss forward (L1- MAE): Regeneration of a source image when used with the other model.</small>
    - <small> Cycle loss backward (L1- MAE): Regeneration of a source image when used with the other model.</small>

    